{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERA IMMAGINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API if not already installed\n",
    "%pip install -q kaggle\n",
    "\n",
    "# Upload your kaggle.json file\n",
    "from google.colab import files\n",
    "files.upload()  # Upload kaggle.json here\n",
    "\n",
    "# Move kaggle.json to the correct location and set permissions\n",
    "%mkdir -p ~/.kaggle\n",
    "%mv kaggle.json ~/.kaggle/\n",
    "%chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download the dataset from Kaggle\n",
    "# Replace 'username/dataset-name' with the actual Kaggle dataset name\n",
    "# !kaggle datasets download -d denislukovnikov/ffhq256-images-only -p /content\n",
    "\n",
    "# Unzip the dataset\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "dataset_zip_path = '/content/ffhq256-images-only.zip'  # Change this to the actual downloaded file name\n",
    "dataset_extracted_dir = '/content/dataset_images'\n",
    "\n",
    "if not os.path.exists(dataset_extracted_dir):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_extracted_dir)\n",
    "    print(\"Dataset extracted.\")\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")\n",
    "\n",
    "# Now the dataset is available; continue with image processing code\n",
    "\n",
    "import PIL\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
    "\n",
    "# Initialize the model pipeline\n",
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Define output zip paths for storing original and modified images\n",
    "original_zip_path = '/content/original_images.zip'\n",
    "modified_zip_path = '/content/modified_images.zip'\n",
    "\n",
    "# Function to load a local image\n",
    "def load_image_from_local(path):\n",
    "    image = PIL.Image.open(path)\n",
    "    image = PIL.ImageOps.exif_transpose(image)  # Handle orientation based on EXIF data\n",
    "    image = image.convert(\"RGB\")                # Convert to RGB format if needed\n",
    "    return image\n",
    "\n",
    "# Define prompt for style transfer\n",
    "prompt = \"vincent van gogh style\"\n",
    "\n",
    "# Process and save images\n",
    "processed_images = []\n",
    "max_images = 1000  # Adjust max_images if needed\n",
    "count = 0\n",
    "\n",
    "# Open zip files to save images\n",
    "with zipfile.ZipFile(original_zip_path, 'w') as original_zip, zipfile.ZipFile(modified_zip_path, 'w') as modified_zip:\n",
    "    for dirname, _, filenames in os.walk(dataset_extracted_dir):\n",
    "        for filename in filenames:\n",
    "            if count >= max_images:\n",
    "                break\n",
    "\n",
    "            image_path = os.path.join(dirname, filename)\n",
    "            print(f\"Number {count}, Processing image: {image_path}\")\n",
    "\n",
    "            # Load the original image\n",
    "            original_image = load_image_from_local(image_path)\n",
    "\n",
    "            # Save the original image to the zip file\n",
    "            with original_zip.open(filename, 'w') as img_file:\n",
    "                original_image.save(img_file, format='PNG')\n",
    "\n",
    "            # Apply style transfer to the image\n",
    "            modified_images = pipe(prompt, image=original_image).images\n",
    "            modified_image = modified_images[0]\n",
    "\n",
    "            # Save the modified image to the modified zip file\n",
    "            with modified_zip.open(filename, 'w') as img_file:\n",
    "                modified_image.save(img_file, format='PNG')\n",
    "\n",
    "            # Store the modified image for display\n",
    "            processed_images.append(modified_image)\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "        if count >= max_images:\n",
    "            break\n",
    "\n",
    "print(\"Processing complete. Zipped files saved at:\")\n",
    "print(f\"- Original images: {original_zip_path}\")\n",
    "print(f\"- Modified images: {modified_zip_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PIX2PIX TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tensorflow/examples.git\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class AdaptationLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.adaptation = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.adaptation(x)\n",
    "\n",
    "class PyTorchGenerator(nn.Module):\n",
    "    def __init__(self, tf_generator):\n",
    "        super().__init__()\n",
    "        self.tf_generator = tf_generator\n",
    "        \n",
    "        # Add trainable adaptation layers\n",
    "        self.input_adaptation = AdaptationLayer(3)\n",
    "        self.output_adaptation = AdaptationLayer(3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply input adaptation\n",
    "        x = self.input_adaptation(x)\n",
    "        \n",
    "        # Convert PyTorch tensor to TF tensor, with detach\n",
    "        x_tf = tf.convert_to_tensor(x.detach().cpu().numpy().transpose(0, 2, 3, 1))\n",
    "        \n",
    "        # Run through frozen TF model\n",
    "        output_tf = self.tf_generator(x_tf, training=False)\n",
    "        \n",
    "        # Convert back to PyTorch tensor\n",
    "        output_torch = torch.from_numpy(output_tf.numpy().transpose(0, 3, 1, 2))\n",
    "        if x.is_cuda:\n",
    "            output_torch = output_torch.cuda()\n",
    "        \n",
    "        # Apply output adaptation\n",
    "        output_torch = self.output_adaptation(output_torch)\n",
    "        \n",
    "        return output_torch\n",
    "\n",
    "class PyTorchDiscriminator(nn.Module):\n",
    "    def __init__(self, tf_discriminator):\n",
    "        super().__init__()\n",
    "        self.tf_discriminator = tf_discriminator\n",
    "        \n",
    "        # Add trainable adaptation layers\n",
    "        self.input_adaptation = AdaptationLayer(6)  # 6 channels for concatenated input\n",
    "        self.output_adaptation = nn.Conv2d(1, 1, 1)  # 1x1 conv for final output\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # Concatenate input and target/generated images\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        # Apply input adaptation\n",
    "        xy = self.input_adaptation(xy)\n",
    "        \n",
    "        # Convert PyTorch tensor to TF tensor, with detach\n",
    "        xy_tf = tf.convert_to_tensor(xy.detach().cpu().numpy().transpose(0, 2, 3, 1))\n",
    "        \n",
    "        # Split back into x and y for TF discriminator\n",
    "        x_tf = xy_tf[..., :3]\n",
    "        y_tf = xy_tf[..., 3:]\n",
    "        \n",
    "        # Run through frozen TF model\n",
    "        output_tf = self.tf_discriminator([x_tf, y_tf], training=False)\n",
    "        \n",
    "        # Convert back to PyTorch tensor\n",
    "        output_torch = torch.from_numpy(output_tf.numpy()).permute(0, 3, 1, 2)\n",
    "        if x.is_cuda:\n",
    "            output_torch = output_torch.cuda()\n",
    "        \n",
    "        # Apply output adaptation\n",
    "        output_torch = self.output_adaptation(output_torch)\n",
    "        \n",
    "        return output_torch\n",
    "\n",
    "class Pix2PixDataset(Dataset):\n",
    "    def __init__(self, input_folder, target_folder, image_size=(256, 256)):\n",
    "        self.input_paths = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.png')])\n",
    "        self.target_paths = sorted([os.path.join(target_folder, f) for f in os.listdir(target_folder) if f.endswith('.png')])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = Image.open(self.input_paths[idx]).convert('RGB')\n",
    "        target_image = Image.open(self.target_paths[idx]).convert('RGB')\n",
    "        \n",
    "        input_tensor = self.transform(input_image)\n",
    "        target_tensor = self.transform(target_image)\n",
    "        \n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "def train_pix2pix(generator, discriminator, dataloader, num_epochs=100, device='cuda'):\n",
    "    criterion_gan = nn.BCEWithLogitsLoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    \n",
    "    # Only optimize the adaptation layers\n",
    "    optimizer_g = optim.Adam([\n",
    "        {'params': generator.input_adaptation.parameters()},\n",
    "        {'params': generator.output_adaptation.parameters()}\n",
    "    ], lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    optimizer_d = optim.Adam([\n",
    "        {'params': discriminator.input_adaptation.parameters()},\n",
    "        {'params': discriminator.output_adaptation.parameters()}\n",
    "    ], lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_a, real_b) in enumerate(dataloader):\n",
    "            real_a = real_a.to(device)\n",
    "            real_b = real_b.to(device)\n",
    "            batch_size = real_a.size(0)\n",
    "            \n",
    "            # Ground truths\n",
    "            valid = torch.ones((batch_size, 1, 30, 30), requires_grad=False).to(device)\n",
    "            fake = torch.zeros((batch_size, 1, 30, 30), requires_grad=False).to(device)\n",
    "            \n",
    "            # Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            fake_b = generator(real_a)\n",
    "            pred_fake = discriminator(real_a, fake_b)\n",
    "            loss_gan = criterion_gan(pred_fake, valid)\n",
    "            loss_l1 = criterion_l1(fake_b, real_b) * 30\n",
    "            loss_g = loss_gan + loss_l1\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            # Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            pred_real = discriminator(real_a, real_b)\n",
    "            pred_fake = discriminator(real_a, fake_b.detach())\n",
    "            loss_real = criterion_gan(pred_real, valid)\n",
    "            loss_fake = criterion_gan(pred_fake, fake)\n",
    "            loss_d = (loss_real + loss_fake) / 2\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "        print(f\"[Epoch {epoch}/{num_epochs}][D loss: {loss_d.item():.4f}] [G loss: {loss_g.item():.4f}]\")\n",
    "\n",
    "# Load the pre-trained TensorFlow Pix2Pix model\n",
    "def load_tf_model():\n",
    "    generator = pix2pix.unet_generator(output_channels=3, norm_type='instancenorm')\n",
    "    discriminator = pix2pix.discriminator(norm_type='instancenorm', target=True)\n",
    "    return generator, discriminator\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load TF models\n",
    "    tf_generator, tf_discriminator = load_tf_model()\n",
    "    \n",
    "    # Create PyTorch wrapper models with adaptation layers\n",
    "    generator = PyTorchGenerator(tf_generator).to(device)\n",
    "    discriminator = PyTorchDiscriminator(tf_discriminator).to(device)\n",
    "    \n",
    "    # Freeze TF model weights (they're already frozen in the TF model)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = Pix2PixDataset(\n",
    "        input_folder='/kaggle/input/originalimages/original_images',\n",
    "        target_folder='/kaggle/input/modifiedimages/modified_images'\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Train adaptation layers\n",
    "    train_pix2pix(generator, discriminator, dataloader, num_epochs=100, device=device)\n",
    "    \n",
    "    # Save fine-tuned models\n",
    "    torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "    }, 'fine_tuned_models.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
