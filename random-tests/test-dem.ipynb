{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione dipendenze\n",
    "!pip install diffusers transformers torch tqdm pillow\n",
    "\n",
    "# Download dataset di test\n",
    "!mkdir -p /kaggle/working/test_dataset/faces\n",
    "!wget -P /kaggle/working/test_dataset/faces https://upload.wikimedia.org/wikipedia/commons/d/dc/Steve_Jobs_Headshot_2010-CROP_%28cropped_2%29.jpg\n",
    "!wget -P /kaggle/working/test_dataset/faces https://upload.wikimedia.org/wikipedia/commons/1/18/Mark_Zuckerberg_F8_2019_Keynote_%2832830578717%29_%28cropped%29.jpg\n",
    "!wget -P /kaggle/working/test_dataset/faces https://upload.wikimedia.org/wikipedia/commons/3/34/Elon_Musk_Royal_Society_%28crop2%29.jpg\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DatasetGenerator:\n",
    "    def __init__(self, base_dir=\"/kaggle/working/dataset\"):\n",
    "        self.base_dir = base_dir\n",
    "        self.setup_directories()\n",
    "        self.setup_model()\n",
    "        \n",
    "        # Parametri esatti dal paper per IP2P\n",
    "        self.ip2p_params = {\n",
    "            \"num_inference_steps\": 100,  # Dal paper: 100 denoising steps\n",
    "            \"image_guidance_scale\": 1.5,  # Dal paper: image guidance of 1.5\n",
    "            \"guidance_scale\": 7.5        # Dal paper: text guidance of 7.5\n",
    "        }\n",
    "        \n",
    "        # Concetti dal paper\n",
    "        self.concepts = {\n",
    "            \"old_person\": \"make the person look older\",\n",
    "            \"vangogh_style\": \"convert this into Vincent van Gogh painting style\",\n",
    "            \"watercolor\": \"convert this into watercolor painting style\",\n",
    "            \"blond_person\": \"make the person blonde\",\n",
    "            \"tan_person\": \"make the person look tanned\"\n",
    "        }\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"Crea la struttura delle directory\"\"\"\n",
    "        self.dirs = {\n",
    "            \"raw\": os.path.join(self.base_dir, \"raw\"),\n",
    "            \"processed\": os.path.join(self.base_dir, \"processed\")\n",
    "        }\n",
    "        \n",
    "        for dir_path in self.dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            \n",
    "        print(\"Directory create con successo\")\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Carica il modello InstructPix2Pix\"\"\"\n",
    "        try:\n",
    "            self.model = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "                \"timbrooks/instruct-pix2pix\",\n",
    "                torch_dtype=torch.float16,\n",
    "                safety_checker=None\n",
    "            ).to(\"cuda\")\n",
    "            print(\"Modello caricato con successo\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del modello: {e}\")\n",
    "            raise\n",
    "\n",
    "    def process_image(self, image_path, instruction):\n",
    "        \"\"\"Processa una singola immagine con i parametri del paper\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            # Resize a 512x512 come standard per InstructPix2Pix\n",
    "            image = image.resize((256, 256))\n",
    "\n",
    "            result = self.model(\n",
    "                prompt=instruction,\n",
    "                image=image,\n",
    "                **self.ip2p_params  # Usa i parametri esatti del paper\n",
    "            ).images[0]\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel processamento dell'immagine {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_dataset(self, source_dir):\n",
    "        \"\"\"Genera il dataset\"\"\"\n",
    "        print(f\"Inizio processamento immagini da: {source_dir}\")\n",
    "        \n",
    "        # Prendi tutte le immagini dalla directory\n",
    "        image_files = [f for f in os.listdir(source_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        print(f\"Trovate {len(image_files)} immagini\")\n",
    "\n",
    "        # Dizionario per tenere traccia dei risultati\n",
    "        results = {concept: {\"success\": 0, \"failed\": 0} for concept in self.concepts}\n",
    "\n",
    "        # Processa ogni immagine\n",
    "        for img_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "            img_path = os.path.join(source_dir, img_file)\n",
    "            \n",
    "            for concept_name, instruction in self.concepts.items():\n",
    "                # Crea cartella per il concetto\n",
    "                concept_dir = os.path.join(self.dirs[\"processed\"], concept_name)\n",
    "                os.makedirs(concept_dir, exist_ok=True)\n",
    "\n",
    "                # Genera l'immagine modificata\n",
    "                modified_img = self.process_image(img_path, instruction)\n",
    "                \n",
    "                if modified_img:\n",
    "                    # Salva le immagini\n",
    "                    base_name = os.path.splitext(img_file)[0]\n",
    "                    original_path = os.path.join(concept_dir, f\"original_{base_name}.png\")\n",
    "                    modified_path = os.path.join(concept_dir, f\"modified_{base_name}.png\")\n",
    "                    \n",
    "                    Image.open(img_path).convert('RGB').save(original_path)\n",
    "                    modified_img.save(modified_path)\n",
    "                    \n",
    "                    results[concept_name][\"success\"] += 1\n",
    "                    print(f\"Salvate immagini per concetto: {concept_name}\")\n",
    "                else:\n",
    "                    results[concept_name][\"failed\"] += 1\n",
    "\n",
    "        # Stampa risultati finali\n",
    "        print(\"\\nRisultati finali:\")\n",
    "        for concept, stats in results.items():\n",
    "            print(f\"{concept}:\")\n",
    "            print(f\"  Successi: {stats['success']}\")\n",
    "            print(f\"  Falliti: {stats['failed']}\")\n",
    "\n",
    "        print(\"\\nGenerazione dataset completata!\")\n",
    "\n",
    "def check_gpu():\n",
    "    \"\"\"Verifica disponibilit√† GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU disponibile: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria totale: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"GPU non disponibile!\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Verifica GPU\n",
    "    if not check_gpu():\n",
    "        return\n",
    "    \n",
    "    # Crea il generatore\n",
    "    generator = DatasetGenerator()\n",
    "    \n",
    "    # Percorso del dataset di test\n",
    "    test_faces_path = \"/kaggle/working/test_dataset/faces\"\n",
    "    \n",
    "    # Genera dataset\n",
    "    if os.path.exists(test_faces_path):\n",
    "        generator.generate_dataset(test_faces_path)\n",
    "    else:\n",
    "        print(f\"Directory {test_faces_path} non trovata!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"\\nPer visualizzare i risultati:\")\n",
    "print(\"1. Controlla la cartella /kaggle/working/dataset/processed/\")\n",
    "print(\"2. Ogni sottocartella contiene le immagini originali e modificate per ogni concetto\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
