{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9985244,"sourceType":"datasetVersion","datasetId":6144844},{"sourceId":10108793,"sourceType":"datasetVersion","datasetId":6167524},{"sourceId":10108804,"sourceType":"datasetVersion","datasetId":6171916},{"sourceId":188284,"sourceType":"modelInstanceVersion","modelInstanceId":159694,"modelId":182061}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download libraries","metadata":{}},{"cell_type":"code","source":"!pip install torch-fidelity\n!pip install clean-fid","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:09:16.201639Z","iopub.execute_input":"2024-12-09T19:09:16.202386Z","iopub.status.idle":"2024-12-09T19:09:32.465103Z","shell.execute_reply.started":"2024-12-09T19:09:16.202333Z","shell.execute_reply":"2024-12-09T19:09:32.464109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.nn.utils import spectral_norm\nfrom PIL import Image\nimport os\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision.models.resnet import BasicBlock\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nfrom torch.utils.data import ConcatDataset\nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport zipfile\nfrom cleanfid import fid\nfrom torchvision.transforms.functional import to_pil_image\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:09:32.467479Z","iopub.execute_input":"2024-12-09T19:09:32.467895Z","iopub.status.idle":"2024-12-09T19:09:32.475071Z","shell.execute_reply.started":"2024-12-09T19:09:32.467853Z","shell.execute_reply":"2024-12-09T19:09:32.474095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions for GAN","metadata":{}},{"cell_type":"code","source":"# Gaussian Noise Layer\nclass GaussianNoise(nn.Module):\n    def __init__(self, sigma=0.1):\n        super().__init__()\n        self.sigma = sigma\n    \n    def forward(self, x):\n        if self.training:\n            noise = torch.randn_like(x) * self.sigma\n            return x + noise\n        return x\n\n# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n            nn.InstanceNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1, 4, stride=1, padding=1)\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n\nclass TextGuidedGenerator(nn.Module):\n    def __init__(self, input_channels=3, output_channels=3, text_embedding_dim=512, dropout_rate=0.1, device='cuda'):\n        super().__init__()\n        \n        # CLIP text encoder\n        self.clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP parameters\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n            param.requires_grad_(False)\n            \n        # 1. Initial Downsampling + 2 ResNet50\n        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n        self.downsample_resnet = nn.Sequential(\n            resnet.conv1,      # First downsampling\n            resnet.bn1,\n            resnet.relu,\n            resnet.maxpool,    # downsampling\n            resnet.layer1,     # First ResNet block\n            resnet.layer2      # Second ResNet block\n        )\n        for param in self.downsample_resnet.parameters():\n            param.requires_grad = False\n        \n        # 3. DS before transformer\n        self.pre_transformer_ds = nn.Conv2d(512 + text_embedding_dim, 256, 3, stride=2, padding=1)\n        \n        # 4. Transformer block\n        self.transformer = nn.Transformer(\n            d_model=256,\n            nhead=8,\n            num_encoder_layers=3,\n            num_decoder_layers=3,\n            dim_feedforward=1024,\n            dropout=0.1,\n            activation='gelu',\n            batch_first=True\n        )\n        \n        # 5. US after transformer\n        self.post_transformer_us = nn.Sequential(\n            spectral_norm(nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2)\n        )\n        \n        # 6. ResNet block post-transformer\n        self.post_transformer_resnet = BasicBlock(128, 128)\n        \n       # 7. Final Upsampling with  three upsampling steps\n        self.final_upsample = nn.Sequential(\n            # First upsampling\n            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            # Second upsampling\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            # Third upsampling\n            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.LeakyReLU(0.2),\n            # Final Conv\n            nn.Conv2d(32, output_channels, 7, padding=3),\n            nn.Tanh()\n        )\n        \n    def encode_text(self, text):\n        # Get the device from CLIP model\n        device = self.clip_model.device\n        # Tokenize and encode text using CLIP\n        tokens = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n        text_features = self.clip_model(**tokens).last_hidden_state.mean(dim=1)  # Average pooling\n        return text_features\n    \n    def forward(self, x, text_prompt):\n        batch_size = x.size(0)\n        \n        # Text embedding\n        if isinstance(text_prompt, (list, tuple)):\n            text_embedding = self.encode_text(text_prompt)\n        else:\n            text_embedding = self.encode_text([text_prompt] * batch_size)\n            \n        x = self.downsample_resnet(x)\n        \n        # 3. Concatenate text embedding and DS\n        text_embedding = text_embedding.unsqueeze(-1).unsqueeze(-1)\n        text_embedding = text_embedding.expand(-1, -1, x.size(2), x.size(3))\n        \n        x = torch.cat([x, text_embedding], dim=1)\n        \n        x = self.pre_transformer_ds(x)  # 56 -> 28\n        \n        # 4. Transformer\n        b, c, h, w = x.shape\n        x = x.view(b, c, h*w).permute(0, 2, 1)\n        \n        x = self.transformer(x, x)\n        \n        x = x.permute(0, 2, 1).view(b, c, h, w)\n        \n        # 5. Post-transformer US\n        x = self.post_transformer_us(x)\n        \n        # 6. Post-transformer ResNet block\n        x = self.post_transformer_resnet(x)\n        \n        # 7. Final upsampling\n        x = self.final_upsample(x)\n        \n        return x\n        \nclass TextGuidedImageDataset(Dataset):\n    def __init__(self, source_dir, target_dir, text_prompt, transform=None):\n        self.source_dir = source_dir\n        self.target_dir = target_dir\n        self.text_prompt = text_prompt\n        self.transform = transform\n        source_images = set(os.listdir(source_dir))\n        target_images = set(os.listdir(target_dir))\n        self.images = list(source_images.intersection(target_images))\n        if len(self.images) == 0:\n            raise ValueError(\"No matching images found.\")\n        print(f\"Found {len(self.images)} matching images for prompt: {text_prompt}\")\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        source_path = os.path.join(self.source_dir, img_name)\n        target_path = os.path.join(self.target_dir, img_name)\n        source_image = Image.open(source_path).convert('RGB')\n        target_image = Image.open(target_path).convert('RGB')\n        if self.transform:\n            source_image = self.transform(source_image)\n            target_image = self.transform(target_image)\n        return source_image, target_image, str(self.text_prompt)\n\n\ndef train_text_guided_gan(generator, discriminator, train_loader, val_loader, num_epochs, device, save_dir=\"models\"):\n    os.makedirs(save_dir, exist_ok=True)\n    criterion_gan = nn.MSELoss()\n    criterion_pixel = nn.L1Loss()\n    optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0003, betas=(0.5, 0.999), weight_decay=1e-1)\n    optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-1)\n    scheduler_g = CosineAnnealingLR(optimizer_g, T_max=num_epochs, eta_min=1e-6)\n    scheduler_d = CosineAnnealingLR(optimizer_d, T_max=num_epochs, eta_min=1e-6)\n    best_val_loss = float('inf')\n    patience = 10\n    early_stop_counter = 0\n    train_metrics = {'g_loss': [], 'd_loss': [], 'val_loss': []}\n    \n    for epoch in range(num_epochs):\n        generator.train()\n        discriminator.train()\n        total_train_loss = 0\n        \n        for source, target, text_prompt in tqdm(train_loader):\n            batch_size = source.size(0)\n            real = target.to(device)\n            source = source.to(device)\n            # Ensure text_prompt is a list of strings\n            if isinstance(text_prompt, torch.Tensor):\n                text_prompt = text_prompt.tolist()\n            \n            # Train discriminator\n            optimizer_d.zero_grad()\n            fake = generator(source, text_prompt)\n            pred_real = discriminator(real)\n            pred_fake = discriminator(fake.detach())\n            real_labels = torch.ones_like(pred_real) * 0.9\n            fake_labels = torch.zeros_like(pred_fake) * 0.1\n            loss_d_real = criterion_gan(pred_real, real_labels)\n            loss_d_fake = criterion_gan(pred_fake, fake_labels)\n            loss_d = (loss_d_real + loss_d_fake) * 0.5\n            loss_d.backward()\n            optimizer_d.step()\n            \n            # Train generator\n            optimizer_g.zero_grad()\n            pred_fake = discriminator(fake)\n            loss_g_gan = criterion_gan(pred_fake, torch.ones_like(pred_fake))\n            loss_g_pixel = criterion_pixel(fake, real) * 20\n            loss_g = loss_g_gan + loss_g_pixel\n            loss_g.backward()\n            optimizer_g.step()\n            \n            train_metrics['g_loss'].append(loss_g.item())\n            train_metrics['d_loss'].append(loss_d.item())\n        \n        # Validation\n        generator.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for source, target, text_prompt in val_loader:\n                source = source.to(device)\n                target = target.to(device)\n                fake = generator(source, text_prompt)\n                val_loss = criterion_pixel(fake, target).item()\n                total_val_loss += val_loss\n        \n        avg_val_loss = total_val_loss / len(val_loader)\n        train_metrics['val_loss'].append(avg_val_loss)\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}: G_loss={np.mean(train_metrics['g_loss'][-len(train_loader):]):.4f}, \"\n              f\"D_loss={np.mean(train_metrics['d_loss'][-len(train_loader):]):.4f}, Val_loss={avg_val_loss:.4f}\")\n        \n        scheduler_g.step()\n        scheduler_d.step()\n    \n    # Save final model\n    torch.save({\n        'generator': generator.state_dict(),\n        'discriminator': discriminator.state_dict(),\n        'epoch': num_epochs-1,\n        'train_metrics': train_metrics\n    }, os.path.join(save_dir, \"final_model.pth\"))\n    \n    return train_metrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:09:32.476683Z","iopub.execute_input":"2024-12-09T19:09:32.477035Z","iopub.status.idle":"2024-12-09T19:09:32.511209Z","shell.execute_reply.started":"2024-12-09T19:09:32.476997Z","shell.execute_reply":"2024-12-09T19:09:32.510386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize dataset and train GAN","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Define source directory and text-guided target directories with prompts\nsource_dir = '/kaggle/input/multiprompt/original_images_ordered'\nstyle_configs = [\n    ('/kaggle/input/multiprompt/albino', \"convert to albino style\"),\n    ('/kaggle/input/multiprompt/blonde', \"make the hair blonde\"),\n    ('/kaggle/input/multiprompt/gogh', \"apply van gogh style\"),\n    ('/kaggle/input/multiprompt/old', \"make the person old\"),\n    ('/kaggle/input/multiprompt/pink', \"make the hair pink\")]\n    \n# Create datasets with text prompts\ndatasets = []\nfor target_dir, prompt in style_configs:\n    dataset = TextGuidedImageDataset(source_dir, target_dir, prompt, transform)\n    datasets.append(dataset)\n\nfull_dataset = ConcatDataset(datasets)\nprint(f\"Combined dataset contains {len(full_dataset)} images\")\n\ntrain_size = int(0.8 * len(full_dataset))\nval_size = int(0.1 * len(full_dataset))\ntest_size = int(0.1 * len(full_dataset))\ntrain_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n\nprint(len(train_dataset), len(val_dataset), len(test_dataset))\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Initialize models\ngenerator = TextGuidedGenerator(device=device).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Train the model\nstart_time = time.time()\n# train_metrics = train_text_guided_gan(\n#     generator, \n#     discriminator, \n#     train_loader, \n#     val_loader, \n#     num_epochs=100, \n#     device=device\n# )\nend_time = (time.time() - start_time)/60\nprint(f\"Training completed in {end_time:.2f} minutes\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:09:32.513296Z","iopub.execute_input":"2024-12-09T19:09:32.513746Z","iopub.status.idle":"2024-12-09T19:09:34.015390Z","shell.execute_reply.started":"2024-12-09T19:09:32.513708Z","shell.execute_reply":"2024-12-09T19:09:34.014449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions for inference on model","metadata":{}},{"cell_type":"code","source":"def load_model(generator, discriminator, checkpoint_path, device):\n    \"\"\"Load model weight from file checkpoint.\"\"\"\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    generator.load_state_dict(checkpoint['generator'])\n    discriminator.load_state_dict(checkpoint['discriminator'])\n    print(f\"Model loaded from checkpoint: {checkpoint_path}\")\n\ndef process_single_image(image_path, generator, text_prompt, transform, device):\n    \"\"\"Process a single image with G and promtp\"\"\"\n    # Load image\n    image = Image.open(image_path).convert('RGB')\n    original_image = image.copy()\n    \n    # transform\n    image_tensor = transform(image).unsqueeze(0).to(device)  # Aggiungi dimensione batch\n    \n    # Generate image\n    generator.eval()\n    with torch.no_grad():\n        generated_image_tensor = generator(image_tensor, text_prompt)\n    \n    # Convert to PIL\n    generated_image_tensor = generated_image_tensor.squeeze(0).cpu()\n    generated_image = ToPILImage()(torch.clamp((generated_image_tensor + 1) / 2, 0, 1))  # Denormalizzazione\n    \n    return original_image, generated_image\n\ndef visualize_images(original_image, generated_image):\n    \"\"\"Shows original and created images side by side\"\"\"\n    #fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    #axes[0].imshow(original_image)\n    #axes[0].axis(\"off\")\n    #axes[0].set_title(\"Input\")\n    \n    #axes[1].imshow(generated_image)\n    #axes[1].axis(\"off\")\n    #axes[1].set_title(\"Output\")\n    plt.imshow(generated_image)\n    plt.axis('off')\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:14:10.385575Z","iopub.execute_input":"2024-12-09T19:14:10.385956Z","iopub.status.idle":"2024-12-09T19:14:10.393282Z","shell.execute_reply.started":"2024-12-09T19:14:10.385926Z","shell.execute_reply":"2024-12-09T19:14:10.392288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# path of input image\nimage_path = '/kaggle/input/obamaa/obama.jpg'\n\n#checkpoint_path = '/kaggle/working/models/final_model.pth'\ncheckpoint_path = '/kaggle/input/e2gan-v6/pytorch/default/2/e2gan-v6-2'\ntext_prompt = \"make the hair pink\"\n\n#[\"convert to albino style\", \"make the hair blonde\",\"apply van gogh style\", \"make the person old\", \"make the hair pink\"]\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n    \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = TextGuidedGenerator(device=device).to(device)\ndiscriminator = Discriminator().to(device)\n    \nload_model(generator, discriminator, checkpoint_path, device)\n    \n# generate image\nstart_time = time.time()\noriginal_image, generated_image = process_single_image(image_path, generator, text_prompt, transform, device)\nend_time = time.time() - start_time\n\n# milliseconds time\nprint(end_time*1000)\n    \nvisualize_images(original_image, generated_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:19:51.307825Z","iopub.execute_input":"2024-12-09T19:19:51.308429Z","iopub.status.idle":"2024-12-09T19:19:53.225144Z","shell.execute_reply.started":"2024-12-09T19:19:51.308394Z","shell.execute_reply":"2024-12-09T19:19:53.224247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compute FID on Testset","metadata":{}},{"cell_type":"code","source":"def generate_image(original_tensor, generator, text_prompt, device):\n    \"\"\"Generate an image by using the tensor of the original one and the promtp\"\"\"\n    original_tensor = original_tensor.unsqueeze(0).to(device)  # add batch dimension\n    generator.eval()\n    with torch.no_grad():\n        generated_tensor = generator(original_tensor, text_prompt)\n    created = torch.clamp((generated_tensor.squeeze(0).cpu() + 1) / 2, 0, 1)  # denormalize\n    created = created.permute(1, 2, 0).numpy()  # from (C, H, W) to (H, W, C)\n    created = to_pil_image(created)\n    \n    return created\n\n# promtps\nprompts = [\"convert to albino style\", \"make the hair blonde\",\n           \"apply van gogh style\", \"make the person old\",\n           \"make the hair pink\"]\n\nstyles = ['albino', 'blonde', 'gogh', 'old', 'pink']\n\n# list where to insert images for test\ntest0, test1,test2, test3, test4 = [], [], [], [], []\n\nfor source_image, target_image, prompt in test_dataset:\n    if prompt == prompts[0]:\n        if len(test0) < 30:\n            test0.append((source_image, target_image, prompt))\n    elif prompt == prompts[1]:\n        if len(test1) < 30:\n            test1.append((source_image, target_image, prompt))\n    elif prompt == prompts[2]:\n        if len(test2) < 30:\n            test2.append((source_image, target_image, prompt))\n    elif prompt == prompts[3]:\n        if len(test3) < 30:\n            test3.append((source_image, target_image, prompt))\n    elif prompt == prompts[4]:\n        if len(test4) < 30:\n            test4.append((source_image, target_image, prompt))\n\nprint(len(test0),len(test1),len(test2),len(test3),len(test4))\n\ntest_lists = [test0, test1, test2, test3, test4]\n\n# Generate image from triple (original, modified, prompt)\nstart_index = 1001\nfor style_idx, prompt in enumerate(prompts):\n    style_folder = styles[style_idx]\n    os.makedirs(f'/kaggle/working/{style_folder}', exist_ok=True)\n\n    for i, (original_tensor, _, _) in enumerate(test_lists[style_idx]):\n\n        # Generate image\n        generated_image = generate_image(original_tensor, generator, prompt, device)\n\n        # Save\n        save_path = f'/kaggle/working/{style_folder}/{start_index + i}.png'\n        generated_image.save(save_path)\n\n    # Compress (to download)\n    zip_file_path = f'/kaggle/working/{style_folder}.zip'\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        for root, _, files in os.walk(f'/kaggle/working/{style_folder}'):\n            for file in files:\n                zipf.write(os.path.join(root, file), arcname=os.path.join(style_folder, file))\n\nfid_scores = []\n\n# Comoute FID between image generated and image from diffusion\nfor style_idx, style_folder in enumerate(styles):\n    diff_dir = f'/kaggle/working/diff_{style_folder}'\n\n    os.makedirs(diff_dir, exist_ok=True)\n\n    for i, (_, modified_tensor, _) in enumerate(test_lists[style_idx]):\n\n        # Save generated image\n        diff_image_path = os.path.join(diff_dir, f\"diff_{style_folder}_{i}.png\")\n        diff_image = to_pil_image(torch.clamp((modified_tensor + 1) / 2, 0, 1))  # Denormalizzazione\n        diff_image.save(diff_image_path)\n\n    # compute FID for every style\n    score = fid.compute_fid(diff_dir, f'/kaggle/working/{style_folder}')\n    fid_scores.append(score)\n    print(f\"FID for {style_folder}: {score}\")\n\n    # clean dir\n    shutil.rmtree(diff_dir)\n\n# final FID\nmedian_fid = np.median(fid_scores)\nprint(f\"The overall FID score is {median_fid}\")\nprint(fid_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:09:36.193695Z","iopub.execute_input":"2024-12-09T19:09:36.194330Z","iopub.status.idle":"2024-12-09T19:11:00.446409Z","shell.execute_reply.started":"2024-12-09T19:09:36.194289Z","shell.execute_reply":"2024-12-09T19:11:00.444459Z"}},"outputs":[],"execution_count":null}]}