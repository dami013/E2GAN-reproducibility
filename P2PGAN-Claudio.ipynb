{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9903007,"sourceType":"datasetVersion","datasetId":6083648}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{"id":"AA2f6SQl-1zr","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:11.777333Z","iopub.execute_input":"2024-11-14T08:43:11.777657Z","iopub.status.idle":"2024-11-14T08:43:16.934725Z","shell.execute_reply.started":"2024-11-14T08:43:11.777621Z","shell.execute_reply":"2024-11-14T08:43:16.933884Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ImagePairsDataset(Dataset):\n    def __init__(self, originals_dir, filtered_dir, transform=None):\n        self.originals_dir = originals_dir\n        self.filtered_dir = filtered_dir\n        self.transform = transform\n        self.image_names = os.listdir(originals_dir)\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        image_name = self.image_names[idx]\n        original_path = os.path.join(self.originals_dir, image_name)\n        filtered_path = os.path.join(self.filtered_dir, image_name)\n\n        original_image = Image.open(original_path)\n        filtered_image = Image.open(filtered_path)\n\n        if self.transform:\n            original_image = self.transform(original_image)\n            filtered_image = self.transform(filtered_image)\n        \n        return original_image, filtered_image","metadata":{"id":"z8162aWg_ija","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:16.936228Z","iopub.execute_input":"2024-11-14T08:43:16.936645Z","iopub.status.idle":"2024-11-14T08:43:16.944131Z","shell.execute_reply.started":"2024-11-14T08:43:16.936610Z","shell.execute_reply":"2024-11-14T08:43:16.943094Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Funzione per il downsampling\nclass Downsample(nn.Module):\n    def __init__(self, filters, size, apply_batchnorm=True):\n        super(Downsample, self).__init__()\n        layers = [\n            nn.Conv2d(in_channels=filters, out_channels=filters, kernel_size=size, stride=2, padding=1, bias=False)\n        ]\n        if apply_batchnorm:\n            layers.append(nn.BatchNorm2d(filters))\n        \n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# Funzione per l'upsampling\nclass Upsample(nn.Module):\n    def __init__(self, filters, size, apply_dropout=False):\n        super(Upsample, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_channels=filters, out_channels=filters // 2, kernel_size=size, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(filters // 2)\n        ]\n\n        if apply_dropout:\n            layers.append(nn.Dropout(0.5))\n\n        layers.append(nn.ReLU(inplace=True))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"id":"6V69w_aH_nTd","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:16.945361Z","iopub.execute_input":"2024-11-14T08:43:16.945673Z","iopub.status.idle":"2024-11-14T08:43:16.959522Z","shell.execute_reply.started":"2024-11-14T08:43:16.945640Z","shell.execute_reply":"2024-11-14T08:43:16.958692Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ResNet block with skip connections\nclass ResNetBlock(nn.Module):\n    def __init__(self, filters):\n        super(ResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(filters)\n        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(filters)\n\n    def forward(self, x):\n        residual = x\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.bn2(self.conv2(x))\n        x += residual\n        return F.relu(x)\n\n# Simplified Transformer block with self-attention and cross-attention\nclass TransformerBlock(nn.Module):\n    def __init__(self, filters, num_heads=4):\n        super(TransformerBlock, self).__init__()\n        self.self_attention = nn.MultiheadAttention(filters, num_heads)\n        self.cross_attention = nn.MultiheadAttention(filters, num_heads)\n        self.norm1 = nn.LayerNorm(filters)\n        self.norm2 = nn.LayerNorm(filters)\n\n    def forward(self, x):\n        # Reshape for attention (batch, channels, height, width) -> (height*width, batch, channels)\n        b, c, h, w = x.size()\n        x_flat = x.view(b, c, -1).permute(2, 0, 1)  # (h*w, batch, channels)\n\n        # Self-attention\n        x_self_attended, _ = self.self_attention(x_flat, x_flat, x_flat)\n        x = x + x_self_attended.permute(1, 2, 0).view(b, c, h, w)\n\n        # Cross-attention (optional, applied to the same input for simplicity)\n        x_flat = x.view(b, c, -1).permute(2, 0, 1)\n        x_cross_attended, _ = self.cross_attention(x_flat, x_flat, x_flat)\n        x = x + x_cross_attended.permute(1, 2, 0).view(b, c, h, w)\n\n        return x","metadata":{"id":"r2jWiKWN_13B","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:16.961319Z","iopub.execute_input":"2024-11-14T08:43:16.961649Z","iopub.status.idle":"2024-11-14T08:43:16.974999Z","shell.execute_reply.started":"2024-11-14T08:43:16.961618Z","shell.execute_reply":"2024-11-14T08:43:16.974139Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Classe del generatore\nclass Generator(nn.Module):\n    def __init__(self, output_channels):\n        super(Generator, self).__init__()\n\n        # Stack del downsampling\n        self.down_stack = nn.ModuleList([\n            Downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n            Downsample(128, 4),  # (batch_size, 64, 64, 128)\n            Downsample(256, 4),  # (batch_size, 32, 32, 256)\n            Downsample(512, 4),  # (batch_size, 16, 16, 512)\n        ])\n\n        # ResNet blocks\n        self.resnet_block1 = ResNetBlock(512)\n        self.resnet_block2 = ResNetBlock(512)\n\n        # Transformer block\n        self.transformer_block = TransformerBlock(512)\n\n        # Stack dell'upsampling\n        self.up_stack = nn.ModuleList([\n            Upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n            Upsample(512, 4),  # (batch_size, 16, 16, 1024)\n            Upsample(256, 4),  # (batch_size, 32, 32, 512)\n            Upsample(128, 4),  # (batch_size, 64, 64, 256)\n        ])\n\n        # Ultimo layer di upsampling\n        self.last = nn.ConvTranspose2d(in_channels=64, out_channels=output_channels, kernel_size=4, stride=2, padding=1)\n\n    def forward(self, x):\n        skips = []\n\n        # Downsampling\n        for down in self.down_stack:\n            x = down(x)\n            skips.append(x)\n\n        # ResNet blocks\n        x = self.resnet_block1(x)\n        x = self.resnet_block2(x)\n\n        # Transformer block\n        x = self.transformer_block(x)\n\n        # Upsampling with skip connections\n        skips = skips[::-1]\n        for up, skip in zip(self.up_stack, skips):\n            x = up(x)\n            x = torch.cat((x, skip), dim=1)\n\n        # Final output layer\n        x = self.last(x)\n        x = torch.tanh(x)  # Output activation\n\n        return x\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels=3):\n        super(Discriminator, self).__init__()\n\n        # Downsampling layers (adjusted to accept concatenated input and target channels)\n        self.down1 = Downsample(input_channels * 2, 64, apply_batchnorm=False)\n        self.down2 = Downsample(64, 128)\n        self.down3 = Downsample(128, 256)\n        self.down4 = Downsample(256, 512)  # Additional downsampling for extra capacity\n\n        # Additional layers\n        self.zero_pad1 = nn.ZeroPad2d(1)\n        self.conv = nn.Conv2d(512, 512, kernel_size=4, stride=1, padding=0, bias=False)\n        self.batchnorm1 = nn.BatchNorm2d(512)\n        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n\n        self.zero_pad2 = nn.ZeroPad2d(1)\n        self.last = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n        self.sigmoid = nn.Sigmoid()  # Final activation layer to constrain output\n\n    def forward(self, inp, tar):\n        # Concatenate input and target along channels\n        x = torch.cat([inp, tar], dim=1)  # Final dimension: (batch_size, height, width, channels*2)\n\n        # Downsampling\n        x = self.down1(x)  # (batch_size, 128, 128, 64)\n        x = self.down2(x)  # (batch_size, 64, 64, 128)\n        x = self.down3(x)  # (batch_size, 32, 32, 256)\n        x = self.down4(x)  # (batch_size, 16, 16, 512)\n\n        # Convolutional layers with batch normalization and activation\n        x = self.zero_pad1(x)  # (batch_size, 18, 18, 512)\n        x = self.conv(x)       # (batch_size, 15, 15, 512)\n        x = self.batchnorm1(x)\n        x = self.leaky_relu(x)\n\n        # Final layers\n        x = self.zero_pad2(x)  # (batch_size, 17, 17, 512)\n        x = self.last(x)       # (batch_size, 14, 14, 1)\n        x = self.sigmoid(x)    # Apply sigmoid to constrain output to [0, 1]\n\n        return x","metadata":{"id":"37bgetv8_5nK","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:16.976278Z","iopub.execute_input":"2024-11-14T08:43:16.976568Z","iopub.status.idle":"2024-11-14T08:43:16.994169Z","shell.execute_reply.started":"2024-11-14T08:43:16.976536Z","shell.execute_reply":"2024-11-14T08:43:16.993182Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class GANTrainer:\n    def __init__(self, generator, discriminator, device, lr=2e-4):\n        self.generator = generator.to(device)\n        self.discriminator = discriminator.to(device)\n        self.device = device\n\n        # Optimizers for generator and discriminator\n        self.optimizer_G = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n        self.optimizer_D = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n        # Loss function\n        self.criterion = nn.BCELoss()\n\n\n    def train_epoch(self, train_loader, epoch):\n        self.generator.train()\n        self.discriminator.train()\n\n        total_loss_G = 0\n        total_loss_D = 0\n\n        for batch_idx, (original, filtered) in enumerate(train_loader):\n            original, filtered = original.to(self.device), filtered.to(self.device)\n\n            # Train Discriminator\n            self.optimizer_D.zero_grad()\n            # Real labels = 1, fake labels = 0\n            real_labels = torch.ones(original.size(0), 1, 1, 1, device=self.device)\n            fake_labels = torch.zeros(original.size(0), 1, 1, 1, device=self.device)\n\n            # Discriminator loss on real images\n            real_output = self.discriminator(original, filtered)\n            loss_real = self.criterion(real_output, real_labels)\n\n            # Generate fake images\n            fake_images = self.generator(original)\n\n            # Discriminator loss on fake images\n            fake_output = self.discriminator(original, fake_images.detach())\n            loss_fake = self.criterion(fake_output, fake_labels)\n\n            # Combine losses and update discriminator\n            loss_D = (loss_real + loss_fake) / 2\n            loss_D.backward()\n            self.optimizer_D.step()\n            total_loss_D += loss_D.item()\n\n            # Train Generator\n            self.optimizer_G.zero_grad()\n\n            # Generator loss (fooling the discriminator)\n            fake_output = self.discriminator(original, fake_images)\n            loss_G = self.criterion(fake_output, real_labels)\n            loss_G.backward()\n            self.optimizer_G.step()\n            total_loss_G += loss_G.item()\n\n        avg_loss_G = total_loss_G / len(train_loader)\n        avg_loss_D = total_loss_D / len(train_loader)\n\n        print(f\"Epoch [{epoch+1}], Loss D: {avg_loss_D:.4f}, Loss G: {avg_loss_G:.4f}\")\n        return avg_loss_D, avg_loss_G\n","metadata":{"id":"h975mp-uDMiP","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:16.995497Z","iopub.execute_input":"2024-11-14T08:43:16.995888Z","iopub.status.idle":"2024-11-14T08:43:17.009317Z","shell.execute_reply.started":"2024-11-14T08:43:16.995855Z","shell.execute_reply":"2024-11-14T08:43:17.008501Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# Function to train the GAN and save models\ndef train_and_save_gan(generator, discriminator, data_loader, epochs=100, save_path=\"gan_model\"):\n    for epoch in range(epochs):\n        for real_imgs, _ in data_loader:\n            batch_size = real_imgs.size(0)\n            valid = torch.ones(batch_size, 1)\n            fake = torch.zeros(batch_size, 1)\n\n            # ---------------------\n            #  Train Generator\n            # ---------------------\n            optimizer_G.zero_grad()\n\n            # Generate noise and create fake images\n            z = torch.randn(batch_size, latent_dim)\n            generated_imgs = generator(z)\n            g_loss = adversarial_loss(discriminator(generated_imgs), valid)\n\n            # Backpropagation for generator\n            g_loss.backward()\n            optimizer_G.step()\n\n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n            optimizer_D.zero_grad()\n\n            # Loss for real images\n            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n\n            # Loss for fake images\n            fake_loss = adversarial_loss(discriminator(generated_imgs.detach()), fake)\n            d_loss = (real_loss + fake_loss) / 2\n\n            # Backpropagation for discriminator\n            d_loss.backward()\n            optimizer_D.step()\n\n        # Print progress every epoch\n        print(f\"Epoch {epoch+1}/{epochs} | D Loss: {d_loss.item()} | G Loss: {g_loss.item()}\")\n        \n        # Salva i checkpoint\n        if (epoch + 1) % 10 == 0:\n            torch.save({\n                'epoch': epoch,\n                'generator_state_dict': generator.state_dict(),\n                'discriminator_state_dict': discriminator.state_dict(),\n                'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n                'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n            }, f'checkpoint_epoch_{epoch+1}.pt')\n\n    # Save the generator and discriminator models\n    torch.save(generator.state_dict(), f\"{save_path}_generator.pth\")\n    torch.save(discriminator.state_dict(), f\"{save_path}_discriminator.pth\")\n    print(f\"Models saved to {save_path}_generator.pth and {save_path}_discriminator.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:17.010691Z","iopub.execute_input":"2024-11-14T08:43:17.010996Z","iopub.status.idle":"2024-11-14T08:43:17.023581Z","shell.execute_reply.started":"2024-11-14T08:43:17.010965Z","shell.execute_reply":"2024-11-14T08:43:17.022719Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Main","metadata":{"id":"85N8Zdsx_6kT"}},{"cell_type":"code","source":"# Set the random seed for reproducibility\ntorch.manual_seed(42)\n\n\n# Determine the device to run on (GPU if available, else CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Transformations to apply to the images\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\n\n# Dataset e DataLoader\noriginals_dir = '/kaggle/input/e2gan-images/original_images'\nfiltered_dir = '/kaggle/input/e2gan-images/modified_images'","metadata":{"id":"YVL5kmLN_lwd","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:17.024598Z","iopub.execute_input":"2024-11-14T08:43:17.024901Z","iopub.status.idle":"2024-11-14T08:43:17.098901Z","shell.execute_reply.started":"2024-11-14T08:43:17.024854Z","shell.execute_reply":"2024-11-14T08:43:17.097933Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Create the dataset and split it into train, validation, and test sets\ndataset = ImagePairsDataset(originals_dir, filtered_dir, transform=transform)\n\n# Split dataset into train (80%), validation (10%), and test (10%)\ntrain_set, val_set, test_set = random_split(dataset, [0.8, 0.1, 0.1])\n\n# DataLoader for loading batches of data\nbatch_size = 32\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=batch_size)\ntest_loader = DataLoader(test_set, batch_size=batch_size)\n\n# Initialize the generator and discriminator models\ngenerator = Generator(output_channels=3).to(device)  # output_channels=3 for RGB images\ndiscriminator = Discriminator(input_channels=3).to(device)  # input_channels=3 for RGB images\n\n# Run the training and save the model\ntrain_and_save_gan(generator, discriminator, train_loader, epochs=10, save_path=\"simple_gan\")","metadata":{"id":"YVL5kmLN_lwd","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:43:17.100046Z","iopub.execute_input":"2024-11-14T08:43:17.100386Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:473: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:473: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# # Initialize the generator and discriminator models\n# generator = Generator(output_channels=3).to(device)  # output_channels=3 for RGB images\n# discriminator = Discriminator(input_channels=3).to(device)  # input_channels=3 for RGB images\n\n\n# # Initialize the GANTrainer class\n# trainer = GANTrainer(generator, discriminator, device)\n\n\n# # Number of epochs to train\n# epochs = 100\n# for epoch in range(epochs):\n#     # Train for one epoch\n#     loss_D, loss_G = trainer.train_epoch(train_loader, epoch)\n\n#     # Save model checkpoints every 10 epochs\n#     if (epoch + 1) % 10 == 0:\n\n#         checkpoint = {\n#             'epoch': epoch,\n#             'generator_state_dict': generator.state_dict(),\n#             'discriminator_state_dict': discriminator.state_dict(),\n#             'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n#             'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n#         }\n\n#         torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pt')\n#         print(f\"Checkpoint saved for epoch {epoch+1}\")\n\n# print(\"Training completed!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}