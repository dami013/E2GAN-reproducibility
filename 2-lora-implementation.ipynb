{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "mount_file_id": "1UA4FCkpi_PauaHg-4MNVQH7eubm4i76b",
   "authorship_tag": "ABX9TyNRI1kUpHy57UNM2Wzn6omm"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 9940410,
     "sourceType": "datasetVersion",
     "datasetId": 6111625
    },
    {
     "sourceId": 10108793,
     "sourceType": "datasetVersion",
     "datasetId": 6167524
    },
    {
     "sourceId": 188923,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 161071,
     "modelId": 183456
    }
   ],
   "dockerImageVersionId": 30805,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install peft",
   "metadata": {
    "id": "S2l9c4ZDsvTz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733349811126,
     "user_tz": -60,
     "elapsed": 5319,
     "user": {
      "displayName": "Stefano Andreotti",
      "userId": "12307679062192185134"
     }
    },
    "outputId": "c3bb6f26-5c03-479d-c79f-ff187e4a721a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:04:05.711048Z",
     "iopub.execute_input": "2024-12-09T12:04:05.711463Z",
     "iopub.status.idle": "2024-12-09T12:04:14.142115Z",
     "shell.execute_reply.started": "2024-12-09T12:04:05.711432Z",
     "shell.execute_reply": "2024-12-09T12:04:14.140992Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport copy\nimport shutil\n\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision.models.resnet import BasicBlock\n\nfrom torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim import AdamW\n\nfrom torch.nn.utils import spectral_norm\n\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\nfrom transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel\nfrom sklearn.cluster import KMeans\nfrom torchvision.transforms import ToPILImage\nfrom PIL import Image",
   "metadata": {
    "id": "rkStgZ_2gDy2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733349847108,
     "user_tz": -60,
     "elapsed": 411,
     "user": {
      "displayName": "Stefano Andreotti",
      "userId": "12307679062192185134"
     }
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:04:14.144076Z",
     "iopub.execute_input": "2024-12-09T12:04:14.144428Z",
     "iopub.status.idle": "2024-12-09T12:04:14.151866Z",
     "shell.execute_reply.started": "2024-12-09T12:04:14.144398Z",
     "shell.execute_reply": "2024-12-09T12:04:14.150960Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## K-means\nUsed to reduce the dataset to take only the most meaningful images with prompt",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ImageClusterSelector:\n    def __init__(self, n_clusters, input_dir, output_dir, device=None):\n        \"\"\"\n        Initialize the image cluster selector.\n        \n        Args:\n            n_clusters (int): Number of clusters for K-means\n            input_dir (str): Directory containing input images\n            output_dir (str): Directory to save selected images\n            device (str, optional): Device to use for computation\n        \"\"\"\n        self.n_clusters = n_clusters\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Initialize CLIP model\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Image preprocessing\n        self.preprocess = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711)\n            )\n        ])\n\n    def extract_embedding(self, image_path):\n        \"\"\"Extract embedding from a single image.\"\"\"\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n            image = self.preprocess(image).unsqueeze(0).to(self.device)\n            with torch.no_grad():\n                embedding = self.clip_model.get_image_features(image)\n            return embedding.cpu().numpy().flatten()\n        except Exception as e:\n            return None\n\n    def get_image_paths(self):\n        \"\"\"Get valid image paths from input directory.\"\"\"\n        valid_extensions = ('.png', '.jpg', '.jpeg', '.webp')\n        image_paths = [os.path.join(self.input_dir, img) \n            for img in os.listdir(self.input_dir) if img.lower().endswith(valid_extensions)]\n        if not image_paths:\n            raise ValueError(f\"No valid images found in {self.input_dir}\")\n        return image_paths\n\n    def process_images(self):\n        \"\"\"Main processing pipeline.\"\"\"\n        try:\n            # Get image paths\n            image_paths = self.get_image_paths()\n            \n            # Extract embeddings\n            embeddings_list = []\n            valid_paths = []\n            \n            for img_path in tqdm(image_paths, desc=\"Processing images\"):\n                embedding = self.extract_embedding(img_path)\n                if embedding is not None:\n                    embeddings_list.append(embedding)\n                    valid_paths.append(img_path)\n            \n            embeddings = np.array(embeddings_list)\n            \n            # Adjust n_clusters if necessary\n            self.n_clusters = min(self.n_clusters, len(valid_paths))\n\n            # Perform clustering\n            kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n            kmeans.fit(embeddings)\n\n            selected_images = []\n            for cluster_id in range(self.n_clusters):\n                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n                cluster_embeddings = embeddings[cluster_indices]\n                distances = np.linalg.norm(\n                    cluster_embeddings - kmeans.cluster_centers_[cluster_id],\n                    axis=1)\n                closest_index = cluster_indices[np.argmin(distances)]\n                selected_images.append(valid_paths[closest_index])\n\n            # Create output directory if it doesn't exist\n            os.makedirs(self.output_dir, exist_ok=True)\n            #Copy selected images\n            for img_path in selected_images:\n                image_name = os.path.basename(img_path)\n                output_path = os.path.join(self.output_dir, image_name)\n                shutil.copy2(img_path, output_path)\n            print(f\"Selected {len(selected_images)} images\\n\")\n            return selected_images\n\n        except Exception as e:\n            raise\n\ndef process_all_folders_in_directory(parent_dir, n_clusters, output_dir):\n    \"\"\"\n    Process all subfolders inside a parent directory, creating a zip file for each.\n\n    Args:\n        parent_dir (str): Parent directory containing folders to process\n        n_clusters (int): Number of clusters for image selection\n        output_dir (str): Directory to save output zip files\n    \"\"\"\n    folders = [f for f in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, f))]\n    folders.remove('original_images_ordered')\n    \n    for folder in folders:\n        input_dir = os.path.join(parent_dir, folder)\n        folder_output_dir = os.path.join(output_dir, folder)\n        os.makedirs(folder_output_dir, exist_ok=True)\n        \n        print(f\"Processing folder: {folder}\")\n        selector = ImageClusterSelector(\n            n_clusters=n_clusters,\n            input_dir=input_dir,\n            output_dir=folder_output_dir\n        )\n        selector.process_images()\n\n# Main script\nif __name__ == \"__main__\":\n    parent_directory = \"/kaggle/input/multiprompt\"  # Parent directory containing subfolders\n    output_directory = \"/kaggle/working/images/\"  # Directory to save the zip files\n    n_clusters = 400  # Number of clusters for image selection\n\n    os.makedirs(output_directory, exist_ok=True)\n    process_all_folders_in_directory(parent_directory, n_clusters, output_directory)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:04:14.153063Z",
     "iopub.execute_input": "2024-12-09T12:04:14.153368Z",
     "iopub.status.idle": "2024-12-09T12:07:07.739977Z",
     "shell.execute_reply.started": "2024-12-09T12:04:14.153342Z",
     "shell.execute_reply": "2024-12-09T12:07:07.739048Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LoRA\n",
    "Implementing LoRA with peft module following the paper indications, adding it to the downsampling, transformer and upsampling blocks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Gaussian Noise Layer\nclass GaussianNoise(nn.Module):\n    def __init__(self, sigma=0.1):\n        super().__init__()\n        self.sigma = sigma\n\n    def forward(self, x):\n        if self.training:\n            noise = torch.randn_like(x) * self.sigma\n            return x + noise\n        return x\n\n# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n            nn.InstanceNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1, 4, stride=1, padding=1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass TextGuidedGenerator(nn.Module):\n    def __init__(self, input_channels=3, output_channels=3, text_embedding_dim=512, dropout_rate=0.1, device='cuda'):\n        super().__init__()\n        \n        # CLIP text encoder\n        self.clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP parameters\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n            param.requires_grad_(False)\n            \n        # 1. Initial Downsampling + 2 ResNet50\n        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n        self.downsample_resnet = nn.Sequential(\n            resnet.conv1,      # First downsampling\n            resnet.bn1,\n            resnet.relu,\n            resnet.maxpool,    # downsampling\n            resnet.layer1,     # First ResNet block\n            resnet.layer2      # Second ResNet block\n        )\n        for param in self.downsample_resnet.parameters():\n            param.requires_grad = False\n        \n        # 3. DS before transformer\n        self.pre_transformer_ds = nn.Conv2d(512 + text_embedding_dim, 256, 3, stride=2, padding=1)\n        \n        # 4. Transformer block\n        self.transformer = nn.Transformer(\n            d_model=256,\n            nhead=8,\n            num_encoder_layers=3,\n            num_decoder_layers=3,\n            dim_feedforward=1024,\n            dropout=0.1,\n            activation='gelu',\n            batch_first=True\n        )\n        \n        # 5. US after transformer\n        self.post_transformer_us = nn.Sequential(\n            spectral_norm(nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2)\n        )\n        \n        # 6. ResNet block post-transformer\n        self.post_transformer_resnet = BasicBlock(128, 128)\n        \n       # 7. Final Upsampling with  three upsampling steps\n        self.final_upsample = nn.Sequential(\n            # First upsampling\n            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            # Second upsampling\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            # Third upsampling\n            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.LeakyReLU(0.2),\n            # Final Conv\n            nn.Conv2d(32, output_channels, 7, padding=3),\n            nn.Tanh()\n        )\n        \n    def encode_text(self, text):\n        # Get the device from CLIP model\n        device = self.clip_model.device\n        # Tokenize and encode text using CLIP\n        tokens = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n        text_features = self.clip_model(**tokens).last_hidden_state.mean(dim=1)  # Average pooling\n        return text_features\n    \n    def forward(self, x, text_prompt):\n        batch_size = x.size(0)\n        \n        # Text embedding\n        if isinstance(text_prompt, (list, tuple)):\n            text_embedding = self.encode_text(text_prompt)\n        else:\n            text_embedding = self.encode_text([text_prompt] * batch_size)\n            \n        x = self.downsample_resnet(x)\n        \n        # 3. Concatenate text embedding and DS\n        text_embedding = text_embedding.unsqueeze(-1).unsqueeze(-1)\n        text_embedding = text_embedding.expand(-1, -1, x.size(2), x.size(3))\n        \n        x = torch.cat([x, text_embedding], dim=1)\n        \n        x = self.pre_transformer_ds(x)  # 56 -> 28\n        \n        # 4. Transformer\n        b, c, h, w = x.shape\n        x = x.view(b, c, h*w).permute(0, 2, 1)\n        \n        x = self.transformer(x, x)\n        \n        x = x.permute(0, 2, 1).view(b, c, h, w)\n        \n        # 5. Post-transformer US\n        x = self.post_transformer_us(x)\n        \n        # 6. Post-transformer ResNet block\n        x = self.post_transformer_resnet(x)\n        \n        # 7. Final upsampling\n        x = self.final_upsample(x)\n        \n        return x\n\nclass TextGuidedImageDataset(Dataset):\n    def __init__(self, source_dir, target_dir, text_prompt, transform=None):\n        self.source_dir = source_dir\n        self.target_dir = target_dir\n        self.text_prompt = text_prompt\n        self.transform = transform\n        source_images = set(os.listdir(source_dir))\n        target_images = set(os.listdir(target_dir))\n        self.images = list(source_images.intersection(target_images))\n        if len(self.images) == 0:\n            raise ValueError(\"No matching images found.\")\n        print(f\"Found {len(self.images)} matching images for prompt: {text_prompt}\")\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        source_path = os.path.join(self.source_dir, img_name)\n        target_path = os.path.join(self.target_dir, img_name)\n        source_image = Image.open(source_path).convert('RGB')\n        target_image = Image.open(target_path).convert('RGB')\n        if self.transform:\n            source_image = self.transform(source_image)\n            target_image = self.transform(target_image)\n        # Ensure text prompt is passed as string\n        return source_image, target_image, str(self.text_prompt)\n\ndef train_text_guided_gan(generator, discriminator, train_loader, val_loader, num_epochs, device, save_dir=\"models\"):\n    os.makedirs(save_dir, exist_ok=True)\n    criterion_gan = nn.MSELoss()\n    criterion_pixel = nn.L1Loss()\n    \n    optimizer_g = torch.optim.AdamW(generator.parameters(), lr=1e-5, betas=(0.5, 0.999), weight_decay=1e-4)\n    optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=1e-5, betas=(0.5, 0.999), weight_decay=1e-4)\n    scheduler_g = CosineAnnealingLR(optimizer_g, T_max=num_epochs, eta_min=1e-6)\n    scheduler_d = CosineAnnealingLR(optimizer_d, T_max=num_epochs, eta_min=1e-6)\n    train_metrics = {'g_loss': [], 'd_loss': [], 'val_loss': []}\n\n    for epoch in range(num_epochs):\n        generator.train()\n        discriminator.train()\n        for source, target, text_prompt in tqdm(train_loader):\n            real = target.to(device)\n            source = source.to(device)\n            # Ensure text_prompt is a list of strings\n            if isinstance(text_prompt, torch.Tensor):\n                text_prompt = text_prompt.tolist()\n\n            # Train discriminator\n            optimizer_d.zero_grad()\n            fake = generator(source, text_prompt)\n            pred_real = discriminator(real)\n            pred_fake = discriminator(fake.detach())\n            real_labels = torch.ones_like(pred_real) * 0.9\n            fake_labels = torch.zeros_like(pred_fake) * 0.1\n            loss_d_real = criterion_gan(pred_real, real_labels)\n            loss_d_fake = criterion_gan(pred_fake, fake_labels)\n            loss_d = (loss_d_real + loss_d_fake) * 0.5\n            loss_d.backward()\n            optimizer_d.step()\n\n            # Train generator\n            optimizer_g.zero_grad()\n            pred_fake = discriminator(fake)\n            loss_g_gan = criterion_gan(pred_fake, torch.ones_like(pred_fake))\n            loss_g_pixel = criterion_pixel(fake, real) * 20\n            loss_g = loss_g_gan + loss_g_pixel\n            loss_g.backward()\n            optimizer_g.step()\n\n            train_metrics['g_loss'].append(loss_g.item())\n            train_metrics['d_loss'].append(loss_d.item())\n\n        # Validation\n        generator.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for source, target, text_prompt in val_loader:\n                source = source.to(device)\n                target = target.to(device)\n                fake = generator(source, text_prompt)\n                val_loss = criterion_pixel(fake, target).item()\n                total_val_loss += val_loss\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        train_metrics['val_loss'].append(avg_val_loss)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}: G_loss={np.mean(train_metrics['g_loss'][-len(train_loader):]):.4f}, \"\n              f\"D_loss={np.mean(train_metrics['d_loss'][-len(train_loader):]):.4f}, Val_loss={avg_val_loss:.4f}\")\n\n        scheduler_g.step()\n        scheduler_d.step()\n\n    # Save final model\n    generator.save_pretrained(save_dir)\n    torch.save({\n        'generator': generator.state_dict(),\n        'discriminator': discriminator.state_dict(),\n        'epoch': num_epochs-1,\n        'train_metrics': train_metrics\n    }, os.path.join(save_dir, \"LoRA_V6.pth\"))\n\n    return generator",
   "metadata": {
    "id": "2V67dgUcf8jF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733351245907,
     "user_tz": -60,
     "elapsed": 443,
     "user": {
      "displayName": "Stefano Andreotti",
      "userId": "12307679062192185134"
     }
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:07:07.743006Z",
     "iopub.execute_input": "2024-12-09T12:07:07.743405Z",
     "iopub.status.idle": "2024-12-09T12:07:07.771953Z",
     "shell.execute_reply.started": "2024-12-09T12:07:07.743363Z",
     "shell.execute_reply": "2024-12-09T12:07:07.771164Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add LoRA\n",
    "Find the layers to apply LoRA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load pre-trained generator model if specified\ncheckpoint_path = \"/kaggle/input/v6/pytorch/default/1/V6.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location='cuda', weights_only=True)\nbase_generator = TextGuidedGenerator()\nbase_generator.load_state_dict(checkpoint['generator'])\n\n#[(n, type(m)) for n, m in base_generator.named_modules()]\nlinear_layers=[]\nrank_numbers=[]\n\nfor n, m in base_generator.named_modules():\n    name = str(type(m))\n    #print(name)\n    if \"Conv2d\" in name and \"clip\" not in name and \"Tanh\" not in name:\n        linear_layers.append(n)\n        \n# Initialize an empty dictionary\nlayer_dict = {}\nalpha_dict = {}\n\n# Populate the dictionary based on the rules\nn_down1=0\nn_down2=0\nn_trans=0\nn_up=0\nfor layer in linear_layers:\n    if (\"downsample_resnet.4\" in layer or \"downsample_resnet\" in layer):\n        layer_dict[layer] = 8#1\n        alpha_dict[layer] = 16\n        n_down1+=1\n    elif \"downsample_resnet.5\" in layer:\n        layer_dict[layer] = 4\n        alpha_dict[layer] = 8\n        n_down2+=1\n    elif \"transformer\" in layer:\n        layer_dict[layer] = 4#1\n        alpha_dict[layer] = 8\n        n_trans+=1\n    elif not (\"downsample_resnet\" in layer):\n        layer_dict[layer] = 8\n        alpha_dict[layer] = 16\n        n_up+=1\n\nprint(f\"Layers first downsampling {n_down1}\")\nprint(f\"Layers second downsampling {n_down2}\")\nprint(f\"Layers into transformer {n_trans}\")\nprint(f\"Layers upsampling {n_up}\") # not distinct upsampling since they have the same LoRA rank",
   "metadata": {
    "id": "-ujABTKtkdLn",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:07:07.773168Z",
     "iopub.execute_input": "2024-12-09T12:07:07.773889Z",
     "iopub.status.idle": "2024-12-09T12:07:09.816237Z",
     "shell.execute_reply.started": "2024-12-09T12:07:07.773859Z",
     "shell.execute_reply": "2024-12-09T12:07:09.815252Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training LoRA Model\n",
    "Apply LoRA to the layers and train the new model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Function to count changed weights\ndef count_changed_weights(model_before, model_after):\n    changed_count = 0\n    total_count = 0\n\n    # Iterate through the parameters of both models\n    for (name1, param1), (name2, param2) in zip(model_before.named_parameters(), model_after.named_parameters()):\n        assert name1 == name2, f\"Parameter mismatch: {name1} vs {name2}\"\n        # Check if weights differ\n        changed_count += torch.sum(param1.data != param2.data).item()\n        total_count += param1.numel()\n    \n    return changed_count, total_count\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Define source directory and text-guided target directories with prompts\nsource_dir = '/kaggle/input/multiprompt/original_images_ordered'\nstyle_configs = [\n    ('/kaggle/working/images/sculpture', \"transform it into a sculpture\"),\n]\n\n# Create datasets with text prompts\ndatasets = []\nfor target_dir, prompt in style_configs:\n    dataset = TextGuidedImageDataset(source_dir, target_dir, prompt, transform)\n    datasets.append(dataset)\n\nfull_dataset = ConcatDataset(datasets)\nprint(f\"Combined dataset contains {len(full_dataset)} images\\n\")\n\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Initialize models\ncheckpoint_path = \"/kaggle/input/v6/pytorch/default/1/V6.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\ndiscriminator = Discriminator()\ndiscriminator.load_state_dict(checkpoint['discriminator'])\ndiscriminator = discriminator.to(device)\nprint(f\"Loaded pre-trained discriminator model from {checkpoint_path}\")\n\n# LoRA Layers\nconfig = LoraConfig(\n    target_modules=linear_layers,\n    init_lora_weights=\"gaussian\",\n    lora_dropout=0.3,\n    rank_pattern=layer_dict,\n    alpha_pattern=alpha_dict,\n)\n\n# Making the LoRA generator\nbase_generator = TextGuidedGenerator().to(device)\nlora_generator = get_peft_model(base_generator, config)\nprint(f\"Loaded pre-trained generator model from {checkpoint_path}\")\nlora_generator = lora_generator.to(device)\nlora_generator.print_trainable_parameters()\n\n# Copy the old model to see the number of weights changed\nold_model = copy.deepcopy(lora_generator)\n\n# Train the model\nprint(\"\\nStarting training...\")\nstart_time = time.time()\nlora_generator = train_text_guided_gan(\n    lora_generator,\n    discriminator,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    device=device,\n    save_dir=\"/kaggle/working/model\"\n)\nend_time = (time.time() - start_time)/60\nprint(f\"Training completed in {end_time:.2f} minutes\")\n\n# Compare the model weights after training\nchanged_weights, total_weights = count_changed_weights(old_model, lora_generator)\nprint(f\"Number of weights changed: {changed_weights} out of {total_weights}\")\nprint(f\"Percentage of weights changed: {(changed_weights / total_weights) * 100:.2f}%\")",
   "metadata": {
    "id": "Lg9Bb5E-hr6K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1733351495428,
     "user_tz": -60,
     "elapsed": 244147,
     "user": {
      "displayName": "Stefano Andreotti",
      "userId": "12307679062192185134"
     }
    },
    "outputId": "ced5d3e2-e93a-4555-fde3-e4a2da6ca293",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:07:09.817454Z",
     "iopub.execute_input": "2024-12-09T12:07:09.817736Z",
     "iopub.status.idle": "2024-12-09T12:27:08.622238Z",
     "shell.execute_reply.started": "2024-12-09T12:07:09.817709Z",
     "shell.execute_reply": "2024-12-09T12:27:08.621163Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Model\n",
    "Test the fine tuned model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def load_model(generator, discriminator, checkpoint_path, device):\n    \"\"\"Load model weights from checkpoint file\"\"\"\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n    discriminator.load_state_dict(checkpoint['discriminator'])\n\n    # Load PEFT fine-tuned weights into the generator\n    peft_model = PeftModel.from_pretrained(generator, \"/kaggle/working/model/\")\n    peft_model.eval()  # Set to evaluation mode\n\n    print(f\"Model loaded from: {checkpoint_path}\")\n    return lora_generator\n\n\ndef process_single_image(image_path, lora_generator,text_prompt, transform, device):\n    \"\"\"process a single image from G and prompt\"\"\"\n    \n    # Load the image\n    image = Image.open(image_path).convert('RGB')\n    original_image = image.copy()\n\n    # Apply transforms\n    image_tensor = transform(image).unsqueeze(0).to(device)\n\n    # Generate image\n    lora_generator.eval()\n    with torch.no_grad():\n        lora_image_tensor = lora_generator(image_tensor, text_prompt)\n\n    lora_image_tensor = lora_image_tensor.squeeze(0).cpu()\n    lora_image = ToPILImage()(torch.clamp((lora_image_tensor + 1) / 2, 0, 1))  # Denormalization\n\n    return lora_image\n\n\ndef visualize_images(original_image, lora_image, prompt):\n    \"\"\"shows original and created image side by side\"\"\"\n    \n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(original_image)\n    axes[0].axis(\"off\")\n    axes[0].set_title(\"Input\")\n\n    axes[1].imshow(lora_image)\n    axes[1].axis(\"off\")\n    axes[1].set_title(\"Lora Model\")\n    \n    plt.title(prompt)\n    plt.show()\n\n\n# Specify the image path\nimage_path = '/kaggle/input/obamaset/obama.jpg'\n\n# Load the image\nimage = Image.open(image_path).convert('RGB')\noriginal_image = image.copy()\n\n# transform\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Initialize models\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndiscriminator = Discriminator().to(device)\n\n# Load generator\ncheckpoint_path = \"/kaggle/working/model/LoRA_V6.pth\"\ntest_gen = TextGuidedGenerator().to(device)\nlora_generator = load_model(test_gen, discriminator, checkpoint_path, device)\n\n# Generare immagine\nlora_generator.eval()\n\nprompt_list = ['Transform it into a sculpture', 'Blonde hair', 'Van gogh style']\n\nfor prompt in prompt_list:\n    lora_image = process_single_image(image_path, lora_generator, prompt, transform, device)\n    visualize_images(original_image, lora_image, prompt)",
   "metadata": {
    "id": "9tYbRPto1DWQ",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:33:02.592564Z",
     "iopub.execute_input": "2024-12-09T12:33:02.592909Z",
     "iopub.status.idle": "2024-12-09T12:33:06.053024Z",
     "shell.execute_reply.started": "2024-12-09T12:33:02.592876Z",
     "shell.execute_reply": "2024-12-09T12:33:06.051974Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Our LoRA Implementation\n",
    "Show that the paper idea works but modified for our model, implement a different multi head attention module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class LoRACompatibleMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n\n        self.dropout = nn.Dropout(dropout)\n        self.scaling = self.head_dim ** -0.5\n\n\n    def forward(self, query, key, value, attn_mask=None):\n        batch_size, seq_length, embed_dim = query.size()\n    \n        # Project to query, key, value\n        q = self.q_proj(query).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.k_proj(key).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.v_proj(value).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n    \n        # Check if LoRA is active\n        if hasattr(self.q_proj, \"lora_A\"):\n            print(\"LoRA weights are active\")\n    \n        # Scaled dot-product attention\n        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n        if attn_mask is not None:\n            attn_weights += attn_mask\n        attn_weights = torch.softmax(attn_weights, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n    \n        # Attention output\n        attn_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, seq_length, embed_dim)\n    \n        # Final projection\n        output = self.out_proj(attn_output)\n        return output\n\n\nclass CustomTransformer(nn.Module):\n    def __init__(self, embed_dim, num_heads, num_layers, dropout=0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=embed_dim,\n                nhead=num_heads,\n                dropout=dropout,\n                activation='gelu',\n                dim_feedforward=1024\n            )\n            for _ in range(num_layers)\n        ])\n        \n        # Replace MultiheadAttention in TransformerEncoderLayer with custom attention\n        for layer in self.layers:\n            layer.self_attn = LoRACompatibleMultiheadAttention(embed_dim, num_heads, dropout)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nclass TextGuidedGeneratorWithLoRA(TextGuidedGenerator):\n    def __init__(self,*args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Define LoRA configuration for the transformer\n        self.lora_config = LoraConfig(\n            r=4,\n            lora_alpha=8,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n            lora_dropout=0.2,\n            bias=\"lora_only\"\n        )\n\n        # Apply LoRA\n        self.transformer = get_peft_model(self.transformer, self.lora_config)\n        \n\n    def disable_adapter(self):\n        \"\"\"Disable LoRA layers by setting their parameters to not require gradients.\"\"\"\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) and \"lora\" in name:\n                module.requires_grad_(False)\n\n\n    def enable_adapter(self):\n        \"\"\"Re-enable LoRA layers by allowing their parameters to require gradients.\"\"\"\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) and \"lora\" in name:\n                module.requires_grad_(True)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:33:10.645258Z",
     "iopub.execute_input": "2024-12-09T12:33:10.645604Z",
     "iopub.status.idle": "2024-12-09T12:33:10.658933Z",
     "shell.execute_reply.started": "2024-12-09T12:33:10.645573Z",
     "shell.execute_reply": "2024-12-09T12:33:10.658025Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "Train the new model on the dataset with distillation loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Instantiate the pre-trained model and fine-tuned model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Define transform\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Define source directory and text-guided target directories with prompts\nsource_dir = '/kaggle/input/multiprompt/original_images_ordered'\nstyle_configs = [\n    ('/kaggle/working/images/sculpture', \"transform it into a sculpture\"),\n]\n\n# Create datasets with text prompts\ndatasets = []\nfor target_dir, prompt in style_configs:\n    dataset = TextGuidedImageDataset(source_dir, target_dir, prompt, transform)\n    datasets.append(dataset)\n\nfull_dataset = ConcatDataset(datasets)\nprint(f\"Combined dataset contains {len(full_dataset)} images\\n\")\n\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Path to the saved pre-trained model weights\npretrained_weights_path = \"/kaggle/input/v6/pytorch/default/1/V6.pth\"\n\n# Load the checkpoint\ncheckpoint = torch.load(pretrained_weights_path, map_location=device, weights_only=True)\n\n# Load the pre-trained model (before fine-tuning)\npretrained_model = TextGuidedGeneratorWithLoRA(device=device).to(device)\npretrained_model.eval()  # Set to evaluation mode to freeze weights\n\n# Load the pre-trained model weights\ntry:\n    # Assuming the generator's weights are saved under the key \"generator_state_dict\"\n    pretrained_model.load_state_dict(checkpoint[\"generator\"], strict=False)\n    print(f\"Pre-trained model weights loaded successfully from {pretrained_weights_path}\")\nexcept KeyError:\n    print(f\"Key 'generator_state_dict' not found in the checkpoint. Make sure the checkpoint contains the correct key.\")\n\n# Fine-tuning model with LoRA\nmodel = TextGuidedGeneratorWithLoRA(device=device).to(device)\nmodel.enable_adapter()\n\n# Define loss functions\ntask_loss_fn = nn.MSELoss()  # Loss for the target task\ndistillation_loss_fn = nn.MSELoss()  # Knowledge regularization loss\n\n# Initialize optimizer for LoRA parameters only\noptimizer = AdamW(\n    [param for name, param in model.named_parameters() if param.requires_grad],\n    lr=1e-4,)\n\n# Training loop\nnum_epochs = 100\nlambda_distill = 0.5  # Weight for the distillation loss\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n\n    for batch in train_loader:\n        source_images, target_images, text_prompts = batch\n        source_images = source_images.to(device)\n        target_images = target_images.to(device)\n\n        # Forward pass through both models\n        with torch.no_grad():\n            pretrained_outputs = pretrained_model(source_images, text_prompts)  # Original outputs\n\n        # LoRA fine-tuned outputs\n        fine_tuned_outputs = model(source_images, text_prompts)  \n\n        # Compute task-specific loss\n        task_loss = task_loss_fn(fine_tuned_outputs, target_images)\n\n        # Compute distillation loss\n        distillation_loss = distillation_loss_fn(fine_tuned_outputs, pretrained_outputs)\n\n        # Combine losses\n        loss = task_loss + lambda_distill * distillation_loss\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            source_images, target_images, text_prompts = batch\n            source_images = source_images.to(device)\n            target_images = target_images.to(device)\n\n            # Forward pass through both models\n            pretrained_outputs = pretrained_model(source_images, text_prompts)\n            fine_tuned_outputs = model(source_images, text_prompts)\n\n            # Compute task and distillation losses\n            task_loss = task_loss_fn(fine_tuned_outputs, target_images)\n            distillation_loss = distillation_loss_fn(fine_tuned_outputs, pretrained_outputs)\n\n            # Combine losses\n            loss = task_loss + lambda_distill * distillation_loss\n            val_loss += loss.item()\n\n    # Logging\n    print(f\"Epoch {epoch+1}/{num_epochs}: Distillation_loss={distillation_loss:.4f}, Val_loss={val_loss:.4f}\")\n\n# Save the fine-tuned model\nsave_path = \"text_guided_generator_lora_with_distillation.pth\"\ntorch.save(model.state_dict(), save_path)\nprint(f\"Model saved to {save_path}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T13:25:25.515440Z",
     "iopub.execute_input": "2024-12-09T13:25:25.515763Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test\n",
    "Test the new fine tuned module to see if it is better"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the model state\ndevice = 'cuda'\nmodel = TextGuidedGeneratorWithLoRA(device=device).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/text_guided_generator_lora_with_distillation.pth\", map_location=device, weights_only=True))\nmodel.eval()  # Set model to evaluation mode\nprint(\"Model loaded successfully.\")\n\n# Load a test image\ntest_image_path = \"/kaggle/input/obamaset/obama.jpg\"\ntest_image = Image.open(test_image_path).convert(\"RGB\")\n\n# Define transformations (same as used in training)\ntransform = transforms.Compose([\n    transforms.ToTensor(),          # Convert to tensor\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n])\ntest_image_tensor = transform(test_image).unsqueeze(0).to(device)  # Add batch dimension\n\n# Define a test text prompt\ntest_prompt = \"Transform it into a sculpture\"\n\n# Pass the image and prompt through the model\nwith torch.no_grad():\n    generated_image = model(test_image_tensor, test_prompt)\n\n# Convert the output tensor to an image\ngenerated_image = generated_image.squeeze(0).cpu()  # Remove batch dimension\ngenerated_image = (generated_image * 0.5 + 0.5).clamp(0, 1)  # Rescale to [0, 1]\ngenerated_image = transforms.ToPILImage()(generated_image)\n\n# Save the result\ngenerated_image.save(\"generated_image.jpg\")\n\n# Define a second test text prompt\ntest_prompt2 = \"van gogh style\"\nwith torch.no_grad():\n    generated_image2 = pretrained_model(test_image_tensor, test_prompt2)\n\n# Convert the output tensor to an image\ngenerated_image2 = generated_image2.squeeze(0).cpu()  # Remove batch dimension\ngenerated_image2 = (generated_image2 * 0.5 + 0.5).clamp(0, 1)  # Rescale to [0, 1]\ngenerated_image2 = transforms.ToPILImage()(generated_image2)\n\n# Save the result\ngenerated_image2.save(\"generated_image2.jpg\")\n\n# Create a figure to display the images side by side\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n# Display the original image\nax[0].imshow(generated_image)\nax[0].axis(\"off\")  # Remove axes for better visualization\nax[0].set_title(\"Sculpture\", fontsize=14)\n\n# Display the generated image\nax[1].imshow(generated_image2)\nax[1].axis(\"off\")  # Remove axes for better visualization\nax[1].set_title(test_prompt2, fontsize=14)\n\n# Show the plot\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-09T12:50:13.992631Z",
     "iopub.execute_input": "2024-12-09T12:50:13.993452Z",
     "iopub.status.idle": "2024-12-09T12:50:17.127543Z",
     "shell.execute_reply.started": "2024-12-09T12:50:13.993411Z",
     "shell.execute_reply": "2024-12-09T12:50:17.126742Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
